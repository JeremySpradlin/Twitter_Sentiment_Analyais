{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA \n",
    "> This notebook contains preprocessing and visualization of the texts in the data.\n",
    "\n",
    "> For the visualization spacy, gensim and pyLDAvis is used.\n",
    "\n",
    "> Spacy: Spacy is an library which is used for advance natural processing. It also contains tokenization, Name Entity Recognition(NER), Parts of Speech tagging(POS), etc\n",
    "\n",
    "> Gensim: Gensim is used for unsupervised topic modelling and  finding similarity.\n",
    "\n",
    "> pyLDAviz: It is used to visualize the topic in the LDA model that has been fitted to large corpus. It creates clusters of similar topics. Two or more clusters overlapping means that they are similar. You can change the number of clusters if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required tools\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_style(\"whitegrid\")\n",
    "# import plotly.express as px\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "# import spacy\n",
    "import re\n",
    "# import pyLDAvis.gensim\n",
    "# from wordcloud import WordCloud\n",
    "# import warnings\n",
    "# warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/erbun/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/lib/python3.9/site-packages/seaborn/_decorators.py:36: FutureWarning: Pass the following variable as a keyword arg: x. From version 0.12, the only valid positional argument will be `data`, and passing other arguments without an explicit keyword will result in an error or misinterpretation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Check for data imbalance\n",
    "sns.countplot(train[\"sentiment\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "textID           0\n",
       "text             1\n",
       "selected_text    1\n",
       "sentiment        0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values\n",
    "train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove punctuations and numbers from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dropna(inplace=True)\n",
    "train = train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_punc(text):\n",
    "  cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
    "  cleaned_text = re.sub(r'[0-9]', r'', cleaned_text)\n",
    "  return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"cleaned_text\"] = np.vectorize(clean_punc)(train[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/erbun/.pyenv/versions/3.9.1/lib/python3.9/runpy.py:127: RuntimeWarning: 'nltk.downloader' found in sys.modules after import of package 'nltk', but prior to execution of 'nltk.downloader'; this may result in unpredictable behaviour\n",
      "  warn(RuntimeWarning(msg))\n",
      "[nltk_data] Downloading package stopwords to /Users/erbun/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!python -m nltk.downloader stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "train[\"cleaned_text\"] = train[\"cleaned_text\"].apply(lambda x : \" \".join([w.lower() for w in x.split() if w not in stop_words and len(w) > 3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train[\"cleaned_text\"].apply(lambda x : x.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/erbun/nltk_data'\n    - '/Users/erbun/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/nltk_data'\n    - '/Users/erbun/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/share/nltk_data'\n    - '/Users/erbun/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m~/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet.zip/wordnet/\u001b[0m\n\n  Searched in:\n    - '/Users/erbun/nltk_data'\n    - '/Users/erbun/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/nltk_data'\n    - '/Users/erbun/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/share/nltk_data'\n    - '/Users/erbun/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-ace7a30dada2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlemming_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlemming_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/lib/python3.9/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   4136\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4137\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4138\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-ace7a30dada2>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlemming_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlemming_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-ace7a30dada2>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordNetLemmatizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlemming_texts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlemma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mlemming_texts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/lib/python3.9/site-packages/nltk/stem/wordnet.py\u001b[0m in \u001b[0;36mlemmatize\u001b[0;34m(self, word, pos)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNOUN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mlemmas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwordnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_morphy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlemmas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlemmas\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;31m# __class__ to something new:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     83\u001b[0m                     \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;31m# Load the corpus.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/lib/python3.9/site-packages/nltk/corpus/util.py\u001b[0m in \u001b[0;36m__load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                 \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}/{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/lib/python3.9/site-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n%s\\n%s\\n%s\\n\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mwordnet\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('wordnet')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mcorpora/wordnet\u001b[0m\n\n  Searched in:\n    - '/Users/erbun/nltk_data'\n    - '/Users/erbun/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/nltk_data'\n    - '/Users/erbun/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/share/nltk_data'\n    - '/Users/erbun/.local/share/virtualenvs/Twitter_Sentiment_Analyais-mWwWiToy/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "lemming_texts = texts.apply(lambda x:[lemma.lemmatize(i) for i in x])\n",
    "lemming_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(lemming_texts)):\n",
    "  lemming_texts[i] = \" \".join(lemming_texts[i])\n",
    "\n",
    "train[\"cleaned_text\"] = lemming_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_text = train[\"cleaned_text\"][train[\"sentiment\"]==\"positive\"].apply(lambda x : x.split())\n",
    "negative_text = train[\"cleaned_text\"][train[\"sentiment\"]==\"negative\"].apply(lambda x: x.split())\n",
    "neutral_text = train[\"cleaned_text\"][train[\"sentiment\"]==\"neutral\"].apply(lambda x:x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge all the lists into one common list\n",
    "positive_text = sum(positive_text, [])\n",
    "negative_text = sum(negative_text, [])\n",
    "neutral_text = sum(neutral_text, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the most common words in positive sentiment\n",
    "freq_pos = nltk.FreqDist(positive_text)\n",
    "pos_df = pd.DataFrame({\n",
    "    \"words\":list(freq_pos.keys()),\n",
    "    \"Count\":list(freq_pos.values())\n",
    "})\n",
    "common_pos = pos_df.nlargest(columns=\"Count\", n=30)\n",
    "fig = px.bar(common_pos, x=\"words\", y=\"Count\", labels={\"words\": \"Words\", \"Count\":\"Frequency\"})\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the most common words in negative sentiment\n",
    "freq_neg = nltk.FreqDist(negative_text)\n",
    "neg_df = pd.DataFrame({\n",
    "    \"words\":list(freq_neg.keys()),\n",
    "    \"Count\":list(freq_neg.values())\n",
    "})\n",
    "common_neg = neg_df.nlargest(columns=\"Count\", n=30)\n",
    "fig = px.bar(common_neg, x=\"words\", y=\"Count\", labels={\"words\": \"Words\", \"Count\": \"Frequency\"})\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the most common words in neutral sentiment\n",
    "freq_ntl = nltk.FreqDist(neutral_text)\n",
    "ntl_df = pd.DataFrame({\n",
    "    \"words\":list(freq_ntl.keys()),\n",
    "    \"Count\":list(freq_ntl.values())\n",
    "})\n",
    "common_ntl = ntl_df.nlargest(columns=\"Count\", n=30)\n",
    "fig = px.bar(common_ntl, x=\"words\", y=\"Count\", labels={\"words\": \"Words\", \"Count\":\"Frequency\"})\n",
    "fig.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words in the texts\n",
    "words = \" \".join([text for text in train[\"cleaned_text\"]])\n",
    "wordclouds = WordCloud(width=900, height=600, random_state=42, max_font_size=110).generate(words)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordclouds, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wordcloud for positive words\n",
    "positive_words = \" \".join([word for word in train[\"cleaned_text\"][train[\"sentiment\"] == \"positive\"]])\n",
    "wordcloud_pos = WordCloud(width=900, height=700, random_state=42, max_font_size=100).generate(positive_words)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(wordcloud_pos)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name Entity Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ners = []\n",
    "for text in train[\"cleaned_text\"].values:\n",
    "  doc = nlp(text)\n",
    "\n",
    "  for entity in doc.ents:\n",
    "    ners.append((entity.text, entity.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_entity_df = pd.DataFrame(ners, columns = [\"Entity Name\", \"Entity Label\"])\n",
    "entity_df = name_entity_df.groupby(by=[\"Entity Name\", \"Entity Label\"]).size().sort_values(ascending=False).reset_index().rename(columns = {0: \"Frequency\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the first 50 entities\n",
    "figure = px.bar(x=entity_df[\"Entity Label\"][:50], y=entity_df[\"Frequency\"][:50]) \n",
    "figure.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Topic Modelling is the task of extracting the main topics from the documents.\n",
    "\n",
    ">For this task LDA is used. LDA is used to classify the text in a document to a particular topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = train[\"cleaned_text\"].apply(lambda x: x.split())\n",
    "texts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the dictionary of words from the document\n",
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the LDA Model \n",
    "import gensim\n",
    "topics = 10\n",
    "lda = gensim.models.ldamodel.LdaModel(corpus, num_topics=topics, id2word=dictionary, passes=15)\n",
    "lda.save(\"LDA_model.gensim\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display = pyLDAvis.gensim.prepare(lda, corpus, dictionary)\n",
    "pyLDAvis.display(display)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
